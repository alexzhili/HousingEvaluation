{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27e09ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-78e4ff0a-7f28-4d49-a30c-b592b72b68bb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.1!hadoop-aws.jar (490ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.901!aws-java-sdk-bundle.jar (80787ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (194ms)\n",
      ":: resolution report :: resolve 5564ms :: artifacts dl 81479ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-78e4ff0a-7f28-4d49-a30c-b592b72b68bb\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (189078kB/416ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 20:59:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://192.168.224.11:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>12 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Etc/GMT</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.38.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>7 days, 5 hours and 3 minutes </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>sparkling-water-root_app-20221103205921-0001</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>2</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>2 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://192.168.224.11:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>null</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.9.7 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  --------------------------------------------\n",
       "H2O_cluster_uptime:         12 secs\n",
       "H2O_cluster_timezone:       Etc/GMT\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.38.0.2\n",
       "H2O_cluster_version_age:    7 days, 5 hours and 3 minutes\n",
       "H2O_cluster_name:           sparkling-water-root_app-20221103205921-0001\n",
       "H2O_cluster_total_nodes:    2\n",
       "H2O_cluster_free_memory:    2 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://192.168.224.11:54321\n",
       "H2O_connection_proxy:       null\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.9.7 final\n",
       "--------------------------  --------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * Sparkling Water Version: 3.38.0.2-1-3.3\n",
      " * H2O name: root\n",
      " * cluster size: 2\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (0,192.168.224.8,54321)\n",
      "  (1,192.168.224.9,54321)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://192.168.224.11:54321 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|██████████████████████████████████████████████████| 100%\n",
      "5 models trained. For more details use the getLeaderboard() method on the AutoML object.\n",
      "Returning leader model and printing info about it below.\n",
      "Model Details\n",
      "===============\n",
      "H2OStackedEnsemble\n",
      "Model Key: StackedEnsemble_AllModels_1_AutoML_1_20221103_205953_c9831601701f\n",
      "\n",
      "Model summary\n",
      "\n",
      "Training metrics\n",
      "RMSLE: 0.3566731304299686\n",
      "Nobs: 5780.0\n",
      "RMSE: 228.51200608399907\n",
      "ResidualDeviance: 3.0181851942380434E8\n",
      "NullDeviance: 6.13603612626122E8\n",
      "MAE: 63.67301663622237\n",
      "MeanResidualDeviance: 52217.73692453362\n",
      "ScoringTime: 1.667509218775E12\n",
      "MSE: 52217.73692453362\n",
      "R2: 0.508121345420263\n",
      "NullDegreesOfFreedom: 5779.0\n",
      "AIC: 79202.09541686333\n",
      "ResidualDegreesOfFreedom: 5776.0\n",
      "\n",
      "Cross validation metrics\n",
      "RMSLE: 0.4530675802818606\n",
      "Nobs: 5780.0\n",
      "RMSE: 291.07635394398284\n",
      "ResidualDeviance: 4.897130653103657E8\n",
      "NullDeviance: 6.138676754658073E8\n",
      "MAE: 80.87444317788187\n",
      "MeanResidualDeviance: 84725.44382532279\n",
      "ScoringTime: 1.667509218664E12\n",
      "MSE: 84725.44382532279\n",
      "R2: 0.2019064828929663\n",
      "NullDegreesOfFreedom: 5779.0\n",
      "AIC: 81999.57918147411\n",
      "ResidualDegreesOfFreedom: 5776.0\n",
      "\n",
      "More info available using methods like:\n",
      "getFeatureImportances(), getScoringHistory(), getCrossValidationScoringHistory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/11/03 21:00:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp7lpqby_q, flavor: spark), fall back to return ['pyspark==3.3.1']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "from pysparkling.ml import H2OAutoML\n",
    "from pysparkling import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import socket\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "mlflow.set_experiment('MLOps_Experiment')\n",
    "#mlflow.pyspark.ml.autolog(log_models=False)\n",
    "\n",
    "\n",
    "def get_sparkSession(appName = 'MLOps'):\n",
    "    spark_master = os.environ.get('SPARK_MASTER') # \"spark://spark-master:7077\" \n",
    "    driver_host = socket.gethostbyname(socket.gethostname()) # setting driver host is important in k8s mode, ortherwise excutors cannot find diver host\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(spark_master)\\\n",
    "        .appName(appName) \\\n",
    "        .config(\"spark.driver.host\", driver_host) \\\n",
    "        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.1') \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        \n",
    "    ACCESS_KEY = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "    SECRET_KEY = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "    MLFLOW_S3_ENDPOINT_URL = os.environ.get('MLFLOW_S3_ENDPOINT_URL')\n",
    "\n",
    "    hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    hadoopConf.set('fs.s3a.access.key', ACCESS_KEY)\n",
    "    hadoopConf.set('fs.s3a.secret.key', SECRET_KEY)\n",
    "    hadoopConf.set(\"fs.s3a.endpoint\", MLFLOW_S3_ENDPOINT_URL)\n",
    "    hadoopConf.set('fs.s3.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "    hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "    hadoopConf.set(\"fs.s3a.path.style.access\", 'true')\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "\n",
    "def clean_impute_dataframe(spark, file_uri, keep_cols, impute_cols, impute_strategy = \"median\"):\n",
    "    \n",
    "    raw_df = spark.read.csv(file_uri ,header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "    base_df = raw_df.select(*keep_cols)\n",
    "\n",
    "    from pyspark.sql.functions import col, translate, when\n",
    "    from pyspark.sql.types import IntegerType\n",
    "\n",
    "    #cast datatypes into doubles & simply remove outliers with price beyond normal ranges\n",
    "    doubles_df= base_df.withColumn(\"price\", translate(col(\"price\"), \"$,\", \"\").cast(\"double\")) \\\n",
    "                            .filter(col(\"price\") > 0).filter(col(\"minimum_nights\") <= 365)\n",
    "\n",
    "    integer_columns = [x.name for x in doubles_df.schema.fields if x.dataType == IntegerType()]\n",
    "\n",
    "    for c in integer_columns:\n",
    "        doubles_df = doubles_df.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "    for c in impute_cols:\n",
    "        doubles_df = doubles_df.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))    \n",
    "\n",
    "    from pyspark.ml.feature import Imputer\n",
    "    imputer = Imputer(strategy=impute_strategy, inputCols=impute_cols, outputCols=impute_cols)\n",
    "    imputer_model = imputer.fit(doubles_df)\n",
    "    imputed_df = imputer_model.transform(doubles_df)\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    file_uri = \"s3://mlops/sf-listings.csv\"\n",
    "\n",
    "\n",
    "    keep_cols = [\n",
    "        \"host_is_superhost\",\n",
    "        \"cancellation_policy\",\n",
    "        \"instant_bookable\",\n",
    "        \"host_total_listings_count\",\n",
    "        \"neighbourhood_cleansed\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"property_type\",\n",
    "        \"room_type\",\n",
    "        \"accommodates\",\n",
    "        \"bathrooms\",\n",
    "        \"bedrooms\",\n",
    "        \"beds\",\n",
    "        \"bed_type\",\n",
    "        \"review_scores_rating\",\n",
    "        \"review_scores_accuracy\",\n",
    "        \"review_scores_cleanliness\",\n",
    "        \"review_scores_checkin\",\n",
    "        \"review_scores_communication\",\n",
    "        \"review_scores_location\",\n",
    "        \"review_scores_value\",\n",
    "        \"price\"\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    impute_cols = [\n",
    "        \"bedrooms\",\n",
    "        \"bathrooms\",\n",
    "        \"beds\", \n",
    "        \"review_scores_rating\",\n",
    "        \"review_scores_accuracy\",\n",
    "        \"review_scores_cleanliness\",\n",
    "        \"review_scores_checkin\",\n",
    "        \"review_scores_communication\",\n",
    "        \"review_scores_location\",\n",
    "        \"review_scores_value\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    spark = get_sparkSession(appName = 'MLOps')\n",
    "    imputed_df = clean_impute_dataframe(spark, file_uri, keep_cols, impute_cols, impute_strategy = \"median\")\n",
    "    train_df, test_df = imputed_df.randomSplit([.8, .2] , seed=42)\n",
    "\n",
    "    hc = H2OContext.getOrCreate()\n",
    "    with mlflow.start_run(run_name=\"H2O-autoML\") as run:\n",
    "        \n",
    "        automl = H2OAutoML(labelCol=\"price\", convertUnknownCategoricalLevelsToNa=True)\n",
    "        automl.setExcludeAlgos([\"GLM\",\"DeepLearning\"])\n",
    "        automl.setMaxModels(3)\n",
    "        automl.setSortMetric(\"rmse\")\n",
    "\n",
    "        model = automl.fit(train_df)\n",
    "        from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "        pred_df = model.transform(test_df)\n",
    "        regression_evaluator = RegressionEvaluator(labelCol='price', predictionCol=\"prediction\")\n",
    "        rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n",
    "        r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "\n",
    "\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.spark.log_model(model, 'model')\n",
    "    \n",
    "    spark.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(spark_master)\\\n",
    "    .appName(appName) \\\n",
    "    .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.1') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf89fd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.7 (default, Oct 16 2021, 10:16:06) \n",
      "[GCC 10.2.1 20210110] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ":: loading settings :: url = jar:file:/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9975c2f2-4a67-416a-bbcb-f9c68339a7cf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.1!hadoop-aws.jar (954ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar ...\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/spark-3.3.0-bin-hadoop3/python/pyspark/shell.py\", line 36, in <module>\n",
      "    SparkContext._ensure_initialized()\n",
      "  File \"/spark-3.3.0-bin-hadoop3/python/pyspark/context.py\", line 417, in _ensure_initialized\n",
      "    SparkContext._gateway = gateway or launch_gateway(conf)\n",
      "  File \"/spark-3.3.0-bin-hadoop3/python/pyspark/java_gateway.py\", line 103, in launch_gateway\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      ">>> \n",
      ">>> "
     ]
    }
   ],
   "source": [
    "!pyspark --packages org.apache.hadoop:hadoop-aws:3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c970b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75d251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-03 18:28:28--  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.36.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.36.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 870644 (850K) [application/java-archive]\n",
      "Saving to: ‘/root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.1.jar/hadoop-aws-3.3.1.jar’\n",
      "\n",
      "hadoop-aws-3.3.1.ja 100%[===================>] 850.24K  1.01MB/s    in 0.8s    \n",
      "\n",
      "2022-11-03 18:28:30 (1.01 MB/s) - ‘/root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.1.jar/hadoop-aws-3.3.1.jar’ saved [870644/870644]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget -P /root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.1.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar\n",
    "wget -P /root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.901.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar\n",
    "wget -P /root/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27f3478b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HikariCP-2.5.1.jar\r\n",
      "JLargeArrays-1.5.jar\r\n",
      "JTransforms-3.1.jar\r\n",
      "RoaringBitmap-0.9.25.jar\r\n",
      "ST4-4.0.4.jar\r\n",
      "activation-1.1.1.jar\r\n",
      "aircompressor-0.21.jar\r\n",
      "algebra_2.12-2.0.1.jar\r\n",
      "annotations-17.0.0.jar\r\n",
      "antlr-runtime-3.5.2.jar\r\n",
      "antlr4-runtime-4.8.jar\r\n",
      "aopalliance-repackaged-2.6.1.jar\r\n",
      "arpack-2.2.1.jar\r\n",
      "arpack_combined_all-0.1.jar\r\n",
      "arrow-format-7.0.0.jar\r\n",
      "arrow-memory-core-7.0.0.jar\r\n",
      "arrow-memory-netty-7.0.0.jar\r\n",
      "arrow-vector-7.0.0.jar\r\n",
      "audience-annotations-0.5.0.jar\r\n",
      "automaton-1.11-8.jar\r\n",
      "avro-1.11.0.jar\r\n",
      "avro-ipc-1.11.0.jar\r\n",
      "avro-mapred-1.11.0.jar\r\n",
      "blas-2.2.1.jar\r\n",
      "bonecp-0.8.0.RELEASE.jar\r\n",
      "breeze-macros_2.12-1.2.jar\r\n",
      "breeze_2.12-1.2.jar\r\n",
      "cats-kernel_2.12-2.1.1.jar\r\n",
      "chill-java-0.10.0.jar\r\n",
      "chill_2.12-0.10.0.jar\r\n",
      "commons-cli-1.5.0.jar\r\n",
      "commons-codec-1.15.jar\r\n",
      "commons-collections-3.2.2.jar\r\n",
      "commons-collections4-4.4.jar\r\n",
      "commons-compiler-3.0.16.jar\r\n",
      "commons-compress-1.21.jar\r\n",
      "commons-crypto-1.1.0.jar\r\n",
      "commons-dbcp-1.4.jar\r\n",
      "commons-io-2.11.0.jar\r\n",
      "commons-lang-2.6.jar\r\n",
      "commons-lang3-3.12.0.jar\r\n",
      "commons-logging-1.1.3.jar\r\n",
      "commons-math3-3.6.1.jar\r\n",
      "commons-pool-1.5.4.jar\r\n",
      "commons-text-1.9.jar\r\n",
      "compress-lzf-1.1.jar\r\n",
      "core-1.1.2.jar\r\n",
      "curator-client-2.13.0.jar\r\n",
      "curator-framework-2.13.0.jar\r\n",
      "curator-recipes-2.13.0.jar\r\n",
      "datanucleus-api-jdo-4.2.4.jar\r\n",
      "datanucleus-core-4.1.17.jar\r\n",
      "datanucleus-rdbms-4.1.19.jar\r\n",
      "derby-10.14.2.0.jar\r\n",
      "dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\r\n",
      "flatbuffers-java-1.12.0.jar\r\n",
      "generex-1.0.2.jar\r\n",
      "gson-2.2.4.jar\r\n",
      "guava-14.0.1.jar\r\n",
      "hadoop-client-api-3.3.2.jar\r\n",
      "hadoop-client-runtime-3.3.2.jar\r\n",
      "hadoop-shaded-guava-1.1.1.jar\r\n",
      "hadoop-yarn-server-web-proxy-3.3.2.jar\r\n",
      "hive-beeline-2.3.9.jar\r\n",
      "hive-cli-2.3.9.jar\r\n",
      "hive-common-2.3.9.jar\r\n",
      "hive-exec-2.3.9-core.jar\r\n",
      "hive-jdbc-2.3.9.jar\r\n",
      "hive-llap-common-2.3.9.jar\r\n",
      "hive-metastore-2.3.9.jar\r\n",
      "hive-serde-2.3.9.jar\r\n",
      "hive-service-rpc-3.1.2.jar\r\n",
      "hive-shims-0.23-2.3.9.jar\r\n",
      "hive-shims-2.3.9.jar\r\n",
      "hive-shims-common-2.3.9.jar\r\n",
      "hive-shims-scheduler-2.3.9.jar\r\n",
      "hive-storage-api-2.7.2.jar\r\n",
      "hive-vector-code-gen-2.3.9.jar\r\n",
      "hk2-api-2.6.1.jar\r\n",
      "hk2-locator-2.6.1.jar\r\n",
      "hk2-utils-2.6.1.jar\r\n",
      "httpclient-4.5.13.jar\r\n",
      "httpcore-4.4.14.jar\r\n",
      "istack-commons-runtime-3.0.8.jar\r\n",
      "ivy-2.5.0.jar\r\n",
      "jackson-annotations-2.13.3.jar\r\n",
      "jackson-core-2.13.3.jar\r\n",
      "jackson-core-asl-1.9.13.jar\r\n",
      "jackson-databind-2.13.3.jar\r\n",
      "jackson-dataformat-yaml-2.13.3.jar\r\n",
      "jackson-datatype-jsr310-2.13.3.jar\r\n",
      "jackson-mapper-asl-1.9.13.jar\r\n",
      "jackson-module-scala_2.12-2.13.3.jar\r\n",
      "jakarta.annotation-api-1.3.5.jar\r\n",
      "jakarta.inject-2.6.1.jar\r\n",
      "jakarta.servlet-api-4.0.3.jar\r\n",
      "jakarta.validation-api-2.0.2.jar\r\n",
      "jakarta.ws.rs-api-2.1.6.jar\r\n",
      "jakarta.xml.bind-api-2.3.2.jar\r\n",
      "janino-3.0.16.jar\r\n",
      "javassist-3.25.0-GA.jar\r\n",
      "javax.jdo-3.2.0-m3.jar\r\n",
      "javolution-5.5.1.jar\r\n",
      "jaxb-runtime-2.3.2.jar\r\n",
      "jcl-over-slf4j-1.7.32.jar\r\n",
      "jdo-api-3.0.1.jar\r\n",
      "jersey-client-2.34.jar\r\n",
      "jersey-common-2.34.jar\r\n",
      "jersey-container-servlet-2.34.jar\r\n",
      "jersey-container-servlet-core-2.34.jar\r\n",
      "jersey-hk2-2.34.jar\r\n",
      "jersey-server-2.34.jar\r\n",
      "jline-2.14.6.jar\r\n",
      "joda-time-2.10.13.jar\r\n",
      "jodd-core-3.5.2.jar\r\n",
      "jpam-1.1.jar\r\n",
      "json-1.8.jar\r\n",
      "json4s-ast_2.12-3.7.0-M11.jar\r\n",
      "json4s-core_2.12-3.7.0-M11.jar\r\n",
      "json4s-jackson_2.12-3.7.0-M11.jar\r\n",
      "json4s-scalap_2.12-3.7.0-M11.jar\r\n",
      "jsr305-3.0.0.jar\r\n",
      "jta-1.1.jar\r\n",
      "jul-to-slf4j-1.7.32.jar\r\n",
      "kryo-shaded-4.0.2.jar\r\n",
      "kubernetes-client-5.12.2.jar\r\n",
      "kubernetes-model-admissionregistration-5.12.2.jar\r\n",
      "kubernetes-model-apiextensions-5.12.2.jar\r\n",
      "kubernetes-model-apps-5.12.2.jar\r\n",
      "kubernetes-model-autoscaling-5.12.2.jar\r\n",
      "kubernetes-model-batch-5.12.2.jar\r\n",
      "kubernetes-model-certificates-5.12.2.jar\r\n",
      "kubernetes-model-common-5.12.2.jar\r\n",
      "kubernetes-model-coordination-5.12.2.jar\r\n",
      "kubernetes-model-core-5.12.2.jar\r\n",
      "kubernetes-model-discovery-5.12.2.jar\r\n",
      "kubernetes-model-events-5.12.2.jar\r\n",
      "kubernetes-model-extensions-5.12.2.jar\r\n",
      "kubernetes-model-flowcontrol-5.12.2.jar\r\n",
      "kubernetes-model-metrics-5.12.2.jar\r\n",
      "kubernetes-model-networking-5.12.2.jar\r\n",
      "kubernetes-model-node-5.12.2.jar\r\n",
      "kubernetes-model-policy-5.12.2.jar\r\n",
      "kubernetes-model-rbac-5.12.2.jar\r\n",
      "kubernetes-model-scheduling-5.12.2.jar\r\n",
      "kubernetes-model-storageclass-5.12.2.jar\r\n",
      "lapack-2.2.1.jar\r\n",
      "leveldbjni-all-1.8.jar\r\n",
      "libfb303-0.9.3.jar\r\n",
      "libthrift-0.12.0.jar\r\n",
      "log4j-1.2-api-2.17.2.jar\r\n",
      "log4j-api-2.17.2.jar\r\n",
      "log4j-core-2.17.2.jar\r\n",
      "log4j-slf4j-impl-2.17.2.jar\r\n",
      "logging-interceptor-3.12.12.jar\r\n",
      "lz4-java-1.8.0.jar\r\n",
      "mesos-1.4.3-shaded-protobuf.jar\r\n",
      "metrics-core-4.2.7.jar\r\n",
      "metrics-graphite-4.2.7.jar\r\n",
      "metrics-jmx-4.2.7.jar\r\n",
      "metrics-json-4.2.7.jar\r\n",
      "metrics-jvm-4.2.7.jar\r\n",
      "minlog-1.3.0.jar\r\n",
      "netty-all-4.1.74.Final.jar\r\n",
      "netty-buffer-4.1.74.Final.jar\r\n",
      "netty-codec-4.1.74.Final.jar\r\n",
      "netty-common-4.1.74.Final.jar\r\n",
      "netty-handler-4.1.74.Final.jar\r\n",
      "netty-resolver-4.1.74.Final.jar\r\n",
      "netty-tcnative-classes-2.0.48.Final.jar\r\n",
      "netty-transport-4.1.74.Final.jar\r\n",
      "netty-transport-classes-epoll-4.1.74.Final.jar\r\n",
      "netty-transport-classes-kqueue-4.1.74.Final.jar\r\n",
      "netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar\r\n",
      "netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar\r\n",
      "netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar\r\n",
      "netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar\r\n",
      "netty-transport-native-unix-common-4.1.74.Final.jar\r\n",
      "objenesis-3.2.jar\r\n",
      "okhttp-3.12.12.jar\r\n",
      "okio-1.14.0.jar\r\n",
      "opencsv-2.3.jar\r\n",
      "orc-core-1.7.4.jar\r\n",
      "orc-mapreduce-1.7.4.jar\r\n",
      "orc-shims-1.7.4.jar\r\n",
      "oro-2.0.8.jar\r\n",
      "osgi-resource-locator-1.0.3.jar\r\n",
      "paranamer-2.8.jar\r\n",
      "parquet-column-1.12.2.jar\r\n",
      "parquet-common-1.12.2.jar\r\n",
      "parquet-encoding-1.12.2.jar\r\n",
      "parquet-format-structures-1.12.2.jar\r\n",
      "parquet-hadoop-1.12.2.jar\r\n",
      "parquet-jackson-1.12.2.jar\r\n",
      "pickle-1.2.jar\r\n",
      "protobuf-java-2.5.0.jar\r\n",
      "py4j-0.10.9.5.jar\r\n",
      "rocksdbjni-6.20.3.jar\r\n",
      "scala-collection-compat_2.12-2.1.1.jar\r\n",
      "scala-compiler-2.12.15.jar\r\n",
      "scala-library-2.12.15.jar\r\n",
      "scala-parser-combinators_2.12-1.1.2.jar\r\n",
      "scala-reflect-2.12.15.jar\r\n",
      "scala-xml_2.12-1.2.0.jar\r\n",
      "shapeless_2.12-2.3.7.jar\r\n",
      "shims-0.9.25.jar\r\n",
      "slf4j-api-1.7.32.jar\r\n",
      "snakeyaml-1.30.jar\r\n",
      "snappy-java-1.1.8.4.jar\r\n",
      "spark-catalyst_2.12-3.3.0.jar\r\n",
      "spark-core_2.12-3.3.0.jar\r\n",
      "spark-graphx_2.12-3.3.0.jar\r\n",
      "spark-hive-thriftserver_2.12-3.3.0.jar\r\n",
      "spark-hive_2.12-3.3.0.jar\r\n",
      "spark-kubernetes_2.12-3.3.0.jar\r\n",
      "spark-kvstore_2.12-3.3.0.jar\r\n",
      "spark-launcher_2.12-3.3.0.jar\r\n",
      "spark-mesos_2.12-3.3.0.jar\r\n",
      "spark-mllib-local_2.12-3.3.0.jar\r\n",
      "spark-mllib_2.12-3.3.0.jar\r\n",
      "spark-network-common_2.12-3.3.0.jar\r\n",
      "spark-network-shuffle_2.12-3.3.0.jar\r\n",
      "spark-repl_2.12-3.3.0.jar\r\n",
      "spark-sketch_2.12-3.3.0.jar\r\n",
      "spark-sql_2.12-3.3.0.jar\r\n",
      "spark-streaming_2.12-3.3.0.jar\r\n",
      "spark-tags_2.12-3.3.0-tests.jar\r\n",
      "spark-tags_2.12-3.3.0.jar\r\n",
      "spark-unsafe_2.12-3.3.0.jar\r\n",
      "spark-yarn_2.12-3.3.0.jar\r\n",
      "spire-macros_2.12-0.17.0.jar\r\n",
      "spire-platform_2.12-0.17.0.jar\r\n",
      "spire-util_2.12-0.17.0.jar\r\n",
      "spire_2.12-0.17.0.jar\r\n",
      "stax-api-1.0.1.jar\r\n",
      "stream-2.9.6.jar\r\n",
      "super-csv-2.2.0.jar\r\n",
      "threeten-extra-1.5.0.jar\r\n",
      "tink-1.6.1.jar\r\n",
      "transaction-api-1.1.jar\r\n",
      "univocity-parsers-2.9.1.jar\r\n",
      "velocity-1.5.jar\r\n",
      "xbean-asm9-shaded-4.20.jar\r\n",
      "xz-1.8.jar\r\n",
      "zjsonpatch-0.3.0.jar\r\n",
      "zookeeper-3.6.2.jar\r\n",
      "zookeeper-jute-3.6.2.jar\r\n",
      "zstd-jni-1.5.2-1.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls /spark-3.3.0-bin-hadoop3/jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a493e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\r\n",
      "\r\n",
      "#\r\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\r\n",
      "# contributor license agreements.  See the NOTICE file distributed with\r\n",
      "# this work for additional information regarding copyright ownership.\r\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\r\n",
      "# (the \"License\"); you may not use this file except in compliance with\r\n",
      "# the License.  You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "#\r\n",
      "\r\n",
      "# This file is sourced when running various Spark programs.\r\n",
      "# Copy it as spark-env.sh and edit that to configure Spark for your site.\r\n",
      "\r\n",
      "# Options read when launching programs locally with\r\n",
      "# ./bin/run-example or ./bin/spark-submit\r\n",
      "# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\r\n",
      "# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node\r\n",
      "# - SPARK_PUBLIC_DNS, to set the public dns name of the driver program\r\n",
      "\r\n",
      "# Options read by executors and drivers running inside the cluster\r\n",
      "# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node\r\n",
      "# - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program\r\n",
      "# - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data\r\n",
      "# - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos\r\n",
      "\r\n",
      "# Options read in any mode\r\n",
      "# - SPARK_CONF_DIR, Alternate conf dir. (Default: ${SPARK_HOME}/conf)\r\n",
      "# - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).\r\n",
      "# - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)\r\n",
      "# - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)\r\n",
      "\r\n",
      "# Options read in any cluster manager using HDFS\r\n",
      "# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\r\n",
      "\r\n",
      "# Options read in YARN client/cluster mode\r\n",
      "# - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN\r\n",
      "\r\n",
      "# Options for the daemons used in the standalone deploy mode\r\n",
      "# - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname\r\n",
      "# - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master\r\n",
      "# - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. \"-Dx=y\")\r\n",
      "# - SPARK_WORKER_CORES, to set the number of cores to use on this machine\r\n",
      "# - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)\r\n",
      "# - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker\r\n",
      "# - SPARK_WORKER_DIR, to set the working directory of worker processes\r\n",
      "# - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. \"-Dx=y\")\r\n",
      "# - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).\r\n",
      "# - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. \"-Dx=y\")\r\n",
      "# - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. \"-Dx=y\")\r\n",
      "# - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. \"-Dx=y\")\r\n",
      "# - SPARK_DAEMON_CLASSPATH, to set the classpath for all daemons\r\n",
      "# - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers\r\n",
      "\r\n",
      "# Options for launcher\r\n",
      "# - SPARK_LAUNCHER_OPTS, to set config properties and Java options for the launcher (e.g. \"-Dx=y\")\r\n",
      "\r\n",
      "# Generic options for the daemons used in the standalone deploy mode\r\n",
      "# - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)\r\n",
      "# - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)\r\n",
      "# - SPARK_LOG_MAX_FILES Max log files of Spark daemons can rotate to. Default is 5.\r\n",
      "# - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)\r\n",
      "# - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)\r\n",
      "# - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)\r\n",
      "# - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.\r\n",
      "# Options for native BLAS, like Intel MKL, OpenBLAS, and so on.\r\n",
      "# You might get better performance to enable these options if using native BLAS (see SPARK-21305).\r\n",
      "# - MKL_NUM_THREADS=1        Disable multi-threading of Intel MKL\r\n",
      "# - OPENBLAS_NUM_THREADS=1   Disable multi-threading of OpenBLAS\r\n"
     ]
    }
   ],
   "source": [
    "!cat /spark-3.3.0-bin-hadoop3/conf/spark-env.sh.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6ec9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/11/04 15:35:00 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pyspark.ml. If you encounter errors during autologging, try upgrading / downgrading pyspark.ml to a supported version, or try upgrading MLflow.\n",
      "2022/11/04 15:35:01 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '5c589177c61f447fa4f01e223def2c3a', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n",
      "2022/11/04 15:35:02 WARNING mlflow.utils: Truncated the value of the key `VectorAssembler.inputCols`. Truncated value: `['host_is_superhostIndex', 'cancellation_policyIndex', 'instant_bookableIndex', 'neighbourhood_cleansedIndex', 'property_typeIndex', 'room_typeIndex', 'bed_typeIndex', 'host_total_listings_count', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'bedrooms_na', 'bathrooms_na', 'beds_na', ...`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|██████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN: Dropping bad and constant columns: [review_scores_location_na, features.26, bedrooms_na, features.25, features.28, review_scores_accuracy_na, features.27, features.22, features.21, features.24, features.23, review_scores_checkin_na, features.29, review_scores_value_na, review_scores_rating_na, beds_na, review_scores_communication_na, bathrooms_na, review_scores_cleanliness_na, features.30] (field name: train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "===============\n",
      "H2OGBM\n",
      "Model Key: GBM_5223981cc4a7\n",
      "\n",
      "Model summary\n",
      "Number of Trees: 50\n",
      "Number of Internal Trees: 50\n",
      "Model Size in Bytes: 12629\n",
      "Min. Depth: 5\n",
      "Max. Depth: 5\n",
      "Mean Depth: 5.0\n",
      "Min. Leaves: 7\n",
      "Max. Leaves: 17\n",
      "Mean Leaves: 14.94\n",
      "\n",
      "Training metrics\n",
      "RMSLE: 0.04744695329165878\n",
      "Nobs: 5780.0\n",
      "RMSE: 95.03667085054103\n",
      "MAE: 5.7693701874426075\n",
      "MeanResidualDeviance: 9031.968806354076\n",
      "ScoringTime: 1.667576104383E12\n",
      "MSE: 9031.968806354076\n",
      "R2: 0.91492100335378\n",
      "\n",
      "More info available using methods like:\n",
      "getFeatureImportances(), getScoringHistory(), getCrossValidationScoringHistory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/11/04 15:35:15 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpddswksu9, flavor: spark), fall back to return ['pyspark==3.3.1']. Set logging level to DEBUG to see the full traceback.\n",
      "2022/11/04 15:35:22 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp66w9t8js, flavor: spark), fall back to return ['pyspark==3.3.1']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import socket\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from hyperopt import fmin, tpe, Trials, hp\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pysparkling import *\n",
    "\n",
    "mlflow.set_experiment('MLOps_Experiment')\n",
    "mlflow.pyspark.ml.autolog(log_models=False)\n",
    "\n",
    "\n",
    "def get_sparkSession(appName = 'MLOps'):\n",
    "    spark_master = os.environ.get('SPARK_MASTER') # \"spark://spark-master:7077\" \n",
    "    driver_host = socket.gethostbyname(socket.gethostname()) # setting driver host is important in k8s mode, ortherwise excutors cannot find diver host\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(spark_master)\\\n",
    "        .appName(appName) \\\n",
    "        .config(\"spark.driver.host\", driver_host) \\\n",
    "        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.1') \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "            \n",
    "    ACCESS_KEY = 'admin'\n",
    "    SECRET_KEY = 'sample_key'\n",
    "    MLFLOW_S3_ENDPOINT_URL = 'http://s3:9000'\n",
    "\n",
    "    hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    hadoopConf.set('fs.s3a.access.key', ACCESS_KEY)\n",
    "    hadoopConf.set('fs.s3a.secret.key', SECRET_KEY)\n",
    "    hadoopConf.set(\"fs.s3a.endpoint\", MLFLOW_S3_ENDPOINT_URL)\n",
    "    hadoopConf.set('fs.s3.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "    hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "    hadoopConf.set(\"fs.s3a.path.style.access\", 'true')\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "\n",
    "def clean_impute_dataframe(spark, file_uri, keep_cols, impute_cols, impute_strategy = \"median\"):\n",
    "    \n",
    "    raw_df = spark.read.csv(file_uri ,header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "    base_df = raw_df.select(*keep_cols)\n",
    "\n",
    "    from pyspark.sql.functions import col, translate, when\n",
    "    from pyspark.sql.types import IntegerType\n",
    "\n",
    "    #cast datatypes into doubles & simply remove outliers with price beyond normal ranges\n",
    "    doubles_df= base_df.withColumn(\"price\", translate(col(\"price\"), \"$,\", \"\").cast(\"double\")) \\\n",
    "                            .filter(col(\"price\") > 0).filter(col(\"minimum_nights\") <= 365)\n",
    "\n",
    "    integer_columns = [x.name for x in doubles_df.schema.fields if x.dataType == IntegerType()]\n",
    "\n",
    "    for c in integer_columns:\n",
    "        doubles_df = doubles_df.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "    for c in impute_cols:\n",
    "        doubles_df = doubles_df.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))    \n",
    "\n",
    "    from pyspark.ml.feature import Imputer\n",
    "    imputer = Imputer(strategy=impute_strategy, inputCols=impute_cols, outputCols=impute_cols)\n",
    "    imputer_model = imputer.fit(doubles_df)\n",
    "    imputed_df = imputer_model.transform(doubles_df)\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "\n",
    "\n",
    "def run_H2OXGBoost(imputed_df, labelCol=\"price\"):\n",
    "    \n",
    "    train_df, test_df = imputed_df.randomSplit([.8, .2] , seed=42)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #######################\n",
    "    conf = H2OConf().setExternalClusterMode().setH2OCluster(\"sparkling-water\", 54321)\\\n",
    "                    .setClientIp(\"172.22.0.13\").setCloudName(\"test\")\n",
    "    \n",
    "    hc = H2OContext.getOrCreate(conf)\n",
    "    #################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    from pysparkling.ml import H2OGBMRegressor\n",
    "    from pyspark.sql.functions import exp, col, log\n",
    "    train_df, test_df = imputed_df.withColumn(\"label\", col(\"price\")).randomSplit([.8, .2], seed=42)\n",
    "\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Xgboost\") as run:\n",
    "        # Create pipeline\n",
    "        categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "        index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "        string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")    \n",
    "        numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "        assembler_inputs = index_output_cols + numeric_cols\n",
    "        vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "\n",
    "        #params = {\"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 4, \"random_state\": 42, \"missing\": 0}\n",
    "        xgboost = H2OGBMRegressor(labelCol = \"price\" )\n",
    "        stages = [string_indexer, vec_assembler, xgboost]\n",
    "\n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "        # Log pipeline\n",
    "        mlflow.spark.log_model(pipeline_model, \"model\", input_example=train_df.limit(5).toPandas())\n",
    "\n",
    "        # Log parameter\n",
    "        mlflow.log_param(\"label\", \"price\")\n",
    "        mlflow.log_param(\"features\", \"all_features\")\n",
    "\n",
    "        # Create predictions and metrics\n",
    "        pred_df = pipeline_model.transform(test_df)\n",
    "        regression_evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n",
    "        r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "\n",
    "        # Log both metrics\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.spark.log_model(pipeline_model, \"h2o_model\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    file_uri = \"s3://mlops/sf-listings.csv\"\n",
    "\n",
    "\n",
    "    keep_cols = [\n",
    "        \"host_is_superhost\",\n",
    "        \"cancellation_policy\",\n",
    "        \"instant_bookable\",\n",
    "        \"host_total_listings_count\",\n",
    "        \"neighbourhood_cleansed\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"property_type\",\n",
    "        \"room_type\",\n",
    "        \"accommodates\",\n",
    "        \"bathrooms\",\n",
    "        \"bedrooms\",\n",
    "        \"beds\",\n",
    "        \"bed_type\",\n",
    "        \"review_scores_rating\",\n",
    "        \"review_scores_accuracy\",\n",
    "        \"review_scores_cleanliness\",\n",
    "        \"review_scores_checkin\",\n",
    "        \"review_scores_communication\",\n",
    "        \"review_scores_location\",\n",
    "        \"review_scores_value\",\n",
    "        \"price\"\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    impute_cols = [\n",
    "        \"bedrooms\",\n",
    "        \"bathrooms\",\n",
    "        \"beds\", \n",
    "        \"review_scores_rating\",\n",
    "        \"review_scores_accuracy\",\n",
    "        \"review_scores_cleanliness\",\n",
    "        \"review_scores_checkin\",\n",
    "        \"review_scores_communication\",\n",
    "        \"review_scores_location\",\n",
    "        \"review_scores_value\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    spark = get_sparkSession(appName = 'MLOps')\n",
    "    imputed_df = clean_impute_dataframe(spark, file_uri, keep_cols, impute_cols, impute_strategy = \"median\")\n",
    "    \n",
    "    run_H2OXGBoost(imputed_df, labelCol=\"price\")\n",
    "#     run_RandomForestCV(imputed_df, maxBins=40, labelCol=\"price\")\n",
    "#     run_RandomForest_Hyperopt(imputed_df, maxBins=40, labelCol=\"price\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
